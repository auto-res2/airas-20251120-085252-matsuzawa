=== [UV SYNC] Start at Thu Nov 20 10:03:42 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Thu Nov 20 10:03:49 AM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-iter1-Qwen3-0.6B-gsm8k at Thu Nov 20 10:03:49 AM UTC 2025 ===
Launching training subprocess: /home/toma/t-80-8-b-03/_work/airas-20251120-085252-matsuzawa/airas-20251120-085252-matsuzawa/.venv/bin/python3 -u -m src.train run=proposed-iter1-Qwen3-0.6B-gsm8k results_dir=.research/iteration1 mode=trial
[2025-11-20 10:04:14,768][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
[2025-11-20 10:04:22,400][accelerate.utils.modeling][INFO] - Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 342626816 bytes required
  - 1: 311164928 bytes required
  - 2: 311164928 bytes required
  - 3: 311164928 bytes required
  - 4: 311164928 bytes required
  - 5: 311164928 bytes required
  - 6: 311164928 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
trainable params: 1,146,880 || all params: 597,196,800 || trainable%: 0.1920
[2025-11-20 10:04:38,771][transformers.generation.configuration_utils][WARNING] - The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
=== [TRIAL RUN] PASSED for proposed-iter1-Qwen3-0.6B-gsm8k at Thu Nov 20 10:14:54 AM UTC 2025 ===

=== [UV SYNC] Start at Thu Nov 20 10:15:55 UTC 2025 ===
=== [UV SYNC] Finished successfully at Thu Nov 20 10:16:04 UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-iter1-Qwen3-0.6B-gsm8k at Thu Nov 20 10:16:04 UTC 2025 ===
Launching training subprocess: /mnt/home/toma/KRK-039/_work/airas-20251120-085252-matsuzawa/airas-20251120-085252-matsuzawa/.venv/bin/python3 -u -m src.train run=comparative-1-iter1-Qwen3-0.6B-gsm8k results_dir=.research/iteration1 mode=trial
[2025-11-20 19:16:35,094][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
[2025-11-20 19:16:48,928][accelerate.utils.modeling][INFO] - Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 342626816 bytes required
  - 1: 311164928 bytes required
  - 2: 311164928 bytes required
  - 3: 311164928 bytes required
  - 4: 311164928 bytes required
  - 5: 311164928 bytes required
  - 6: 311164928 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
trainable params: 1,146,880 || all params: 597,196,800 || trainable%: 0.1920
[2025-11-20 19:17:04,329][transformers.generation.configuration_utils][WARNING] - The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
=== [TRIAL RUN] PASSED for comparative-1-iter1-Qwen3-0.6B-gsm8k at Thu Nov 20 10:23:29 UTC 2025 ===

=== [UV SYNC] Start at Thu Nov 20 10:25:22 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Thu Nov 20 10:25:27 AM UTC 2025 ===
=== [FULL EXPERIMENT] Start for proposed-iter1-Qwen3-0.6B-gsm8k at Thu Nov 20 10:25:27 AM UTC 2025 ===
Launching training subprocess: /home/toma/pt80-1-a-29/_work/airas-20251120-085252-matsuzawa/airas-20251120-085252-matsuzawa/.venv/bin/python3 -u -m src.train run=proposed-iter1-Qwen3-0.6B-gsm8k results_dir=.research/iteration1 mode=full
